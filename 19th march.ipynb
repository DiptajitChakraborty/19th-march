{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4e6a2d5c-94f1-4ad4-9bbc-f5cc0d0d29f3",
   "metadata": {},
   "source": [
    "Q1. What is Min-Max scaling, and how is it used in data preprocessing? Provide an example to illustrate its\n",
    "application."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "afee113a-eb65-4dd3-bb36-cfa61ad1ff45",
   "metadata": {},
   "source": [
    "Ans: It is a feature scaling technique used for normlisation of the data. The main task if this scaling is it reduces the size of data into 0-1.This technique helps to bring all the features to a similar scale, preventing one feature from dominating others due to its larger magnitude.\n",
    "\n",
    "Here's how min-max scaling is applied in data preprocessing:\n",
    "\n",
    "Identify the feature: Select the numeric feature or column that you want to normalize. Ensure that the feature has a meaningful range and is continuous.\n",
    "\n",
    "Find the minimum and maximum values: Determine the minimum and maximum values of the selected feature in the dataset.\n",
    "\n",
    "Define the scaling range: Decide on the desired range for the scaled feature. The most common range is between 0 and 1, but you can also choose a different range based on your specific requirements.\n",
    "\n",
    "Apply the min-max scaling formula: Normalize each value in the feature using the min-max scaling formula:\n",
    "\n",
    "scaled_value = (value - min_value) / (max_value - min_value)\n",
    "\n",
    "Here, \"value\" refers to the original value of the feature, \"min_value\" is the minimum value of the feature in the dataset, and \"max_value\" is the maximum value of the feature.\n",
    "\n",
    "Repeat the scaling process for all values in the feature: Apply the scaling formula to each value in the feature column to obtain the normalized values.\n",
    "\n",
    "Use the scaled feature for further analysis: Replace the original feature values with the scaled values in your dataset. The scaled feature can now be used for various analysis tasks, such as machine learning algorithms, where having consistent scales across features is important.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0c8d5172-9634-4b99-abf7-02728da0cd1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "##Example\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "eb93a961-28a6-4fb6-b47f-a722e57fbb7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = sns.load_dataset('taxis')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "3fba67dc-320a-4d77-a81d-910af34583f3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>pickup</th>\n",
       "      <th>dropoff</th>\n",
       "      <th>passengers</th>\n",
       "      <th>distance</th>\n",
       "      <th>fare</th>\n",
       "      <th>tip</th>\n",
       "      <th>tolls</th>\n",
       "      <th>total</th>\n",
       "      <th>color</th>\n",
       "      <th>payment</th>\n",
       "      <th>pickup_zone</th>\n",
       "      <th>dropoff_zone</th>\n",
       "      <th>pickup_borough</th>\n",
       "      <th>dropoff_borough</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2019-03-23 20:21:09</td>\n",
       "      <td>2019-03-23 20:27:24</td>\n",
       "      <td>1</td>\n",
       "      <td>1.60</td>\n",
       "      <td>7.0</td>\n",
       "      <td>2.15</td>\n",
       "      <td>0.0</td>\n",
       "      <td>12.95</td>\n",
       "      <td>yellow</td>\n",
       "      <td>credit card</td>\n",
       "      <td>Lenox Hill West</td>\n",
       "      <td>UN/Turtle Bay South</td>\n",
       "      <td>Manhattan</td>\n",
       "      <td>Manhattan</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2019-03-04 16:11:55</td>\n",
       "      <td>2019-03-04 16:19:00</td>\n",
       "      <td>1</td>\n",
       "      <td>0.79</td>\n",
       "      <td>5.0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>9.30</td>\n",
       "      <td>yellow</td>\n",
       "      <td>cash</td>\n",
       "      <td>Upper West Side South</td>\n",
       "      <td>Upper West Side South</td>\n",
       "      <td>Manhattan</td>\n",
       "      <td>Manhattan</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2019-03-27 17:53:01</td>\n",
       "      <td>2019-03-27 18:00:25</td>\n",
       "      <td>1</td>\n",
       "      <td>1.37</td>\n",
       "      <td>7.5</td>\n",
       "      <td>2.36</td>\n",
       "      <td>0.0</td>\n",
       "      <td>14.16</td>\n",
       "      <td>yellow</td>\n",
       "      <td>credit card</td>\n",
       "      <td>Alphabet City</td>\n",
       "      <td>West Village</td>\n",
       "      <td>Manhattan</td>\n",
       "      <td>Manhattan</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2019-03-10 01:23:59</td>\n",
       "      <td>2019-03-10 01:49:51</td>\n",
       "      <td>1</td>\n",
       "      <td>7.70</td>\n",
       "      <td>27.0</td>\n",
       "      <td>6.15</td>\n",
       "      <td>0.0</td>\n",
       "      <td>36.95</td>\n",
       "      <td>yellow</td>\n",
       "      <td>credit card</td>\n",
       "      <td>Hudson Sq</td>\n",
       "      <td>Yorkville West</td>\n",
       "      <td>Manhattan</td>\n",
       "      <td>Manhattan</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2019-03-30 13:27:42</td>\n",
       "      <td>2019-03-30 13:37:14</td>\n",
       "      <td>3</td>\n",
       "      <td>2.16</td>\n",
       "      <td>9.0</td>\n",
       "      <td>1.10</td>\n",
       "      <td>0.0</td>\n",
       "      <td>13.40</td>\n",
       "      <td>yellow</td>\n",
       "      <td>credit card</td>\n",
       "      <td>Midtown East</td>\n",
       "      <td>Yorkville West</td>\n",
       "      <td>Manhattan</td>\n",
       "      <td>Manhattan</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "               pickup             dropoff  passengers  distance  fare   tip  \\\n",
       "0 2019-03-23 20:21:09 2019-03-23 20:27:24           1      1.60   7.0  2.15   \n",
       "1 2019-03-04 16:11:55 2019-03-04 16:19:00           1      0.79   5.0  0.00   \n",
       "2 2019-03-27 17:53:01 2019-03-27 18:00:25           1      1.37   7.5  2.36   \n",
       "3 2019-03-10 01:23:59 2019-03-10 01:49:51           1      7.70  27.0  6.15   \n",
       "4 2019-03-30 13:27:42 2019-03-30 13:37:14           3      2.16   9.0  1.10   \n",
       "\n",
       "   tolls  total   color      payment            pickup_zone  \\\n",
       "0    0.0  12.95  yellow  credit card        Lenox Hill West   \n",
       "1    0.0   9.30  yellow         cash  Upper West Side South   \n",
       "2    0.0  14.16  yellow  credit card          Alphabet City   \n",
       "3    0.0  36.95  yellow  credit card              Hudson Sq   \n",
       "4    0.0  13.40  yellow  credit card           Midtown East   \n",
       "\n",
       "            dropoff_zone pickup_borough dropoff_borough  \n",
       "0    UN/Turtle Bay South      Manhattan       Manhattan  \n",
       "1  Upper West Side South      Manhattan       Manhattan  \n",
       "2           West Village      Manhattan       Manhattan  \n",
       "3         Yorkville West      Manhattan       Manhattan  \n",
       "4         Yorkville West      Manhattan       Manhattan  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "14bdfd9e-d115-464f-b7b5-6928f2368337",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import MinMaxScaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "5658b826-7c24-4a1d-8592-39f2368067cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "min_max = MinMaxScaler()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "4e7c98e2-c5ea-480d-ad43-0ba58ae57b83",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.04359673, 0.04026846, 0.06475904],\n",
       "       [0.02152589, 0.02684564, 0.        ],\n",
       "       [0.0373297 , 0.04362416, 0.07108434],\n",
       "       ...,\n",
       "       [0.11280654, 0.10067114, 0.        ],\n",
       "       [0.03051771, 0.03355705, 0.        ],\n",
       "       [0.10490463, 0.09395973, 0.10120482]])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "min_max.fit_transform(df[['distance','fare','tip']])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "20a74cee-2bdc-490b-94c9-bd11ce5d93b5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead tr th {\n",
       "        text-align: left;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr>\n",
       "      <th></th>\n",
       "      <th>distance</th>\n",
       "      <th>fare</th>\n",
       "      <th>tip</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.043597</td>\n",
       "      <td>0.040268</td>\n",
       "      <td>0.064759</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.021526</td>\n",
       "      <td>0.026846</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.037330</td>\n",
       "      <td>0.043624</td>\n",
       "      <td>0.071084</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.209809</td>\n",
       "      <td>0.174497</td>\n",
       "      <td>0.185241</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.058856</td>\n",
       "      <td>0.053691</td>\n",
       "      <td>0.033133</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6428</th>\n",
       "      <td>0.020436</td>\n",
       "      <td>0.023490</td>\n",
       "      <td>0.031928</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6429</th>\n",
       "      <td>0.510627</td>\n",
       "      <td>0.382550</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6430</th>\n",
       "      <td>0.112807</td>\n",
       "      <td>0.100671</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6431</th>\n",
       "      <td>0.030518</td>\n",
       "      <td>0.033557</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6432</th>\n",
       "      <td>0.104905</td>\n",
       "      <td>0.093960</td>\n",
       "      <td>0.101205</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>6433 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      distance      fare       tip\n",
       "0     0.043597  0.040268  0.064759\n",
       "1     0.021526  0.026846  0.000000\n",
       "2     0.037330  0.043624  0.071084\n",
       "3     0.209809  0.174497  0.185241\n",
       "4     0.058856  0.053691  0.033133\n",
       "...        ...       ...       ...\n",
       "6428  0.020436  0.023490  0.031928\n",
       "6429  0.510627  0.382550  0.000000\n",
       "6430  0.112807  0.100671  0.000000\n",
       "6431  0.030518  0.033557  0.000000\n",
       "6432  0.104905  0.093960  0.101205\n",
       "\n",
       "[6433 rows x 3 columns]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "pd.DataFrame(min_max.fit_transform(df[['distance','fare','tip']]),columns=[['distance','fare','tip']])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43c095ed-4315-4033-a5b9-6ececa6d99b7",
   "metadata": {},
   "source": [
    "Q2. What is the Unit Vector technique in feature scaling, and how does it differ from Min-Max scaling?\n",
    "Provide an example to illustrate its application."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ddf05af7-d3b1-4679-82a5-c75393b69704",
   "metadata": {},
   "source": [
    "Ans: Unit vector is a value that returns the vector having magnitude 1. \n",
    "The unit vector technique (feature scaling or vector normalization) focuses on transforming the feature vector to have a length of 1 while preserving the direction. On the other hand, min-max scaling aims to rescale the feature values to fit within a specific range. The choice between these techniques depends on the specific requirements of your data analysis or machine learning task.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "9dd83e2e-086a-47c7-b33a-d585ec9620f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "df1 = sns.load_dataset('iris')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "5d250976-8b6d-4e37-a17d-cf38d8bdf647",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import normalize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "6ccf5a37-bde4-4ba8-858a-807f560bfcd4",
   "metadata": {},
   "outputs": [],
   "source": [
    "norm = normalize(df1[['sepal_length', 'sepal_width', 'petal_length', 'petal_width']])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "cac7f834-ea9b-4259-828e-9c7ed5607576",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead tr th {\n",
       "        text-align: left;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr>\n",
       "      <th></th>\n",
       "      <th>sepal_length</th>\n",
       "      <th>sepal_width</th>\n",
       "      <th>petal_length</th>\n",
       "      <th>petal_width</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.803773</td>\n",
       "      <td>0.551609</td>\n",
       "      <td>0.220644</td>\n",
       "      <td>0.031521</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.828133</td>\n",
       "      <td>0.507020</td>\n",
       "      <td>0.236609</td>\n",
       "      <td>0.033801</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.805333</td>\n",
       "      <td>0.548312</td>\n",
       "      <td>0.222752</td>\n",
       "      <td>0.034269</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.800030</td>\n",
       "      <td>0.539151</td>\n",
       "      <td>0.260879</td>\n",
       "      <td>0.034784</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.790965</td>\n",
       "      <td>0.569495</td>\n",
       "      <td>0.221470</td>\n",
       "      <td>0.031639</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>145</th>\n",
       "      <td>0.721557</td>\n",
       "      <td>0.323085</td>\n",
       "      <td>0.560015</td>\n",
       "      <td>0.247699</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>146</th>\n",
       "      <td>0.729654</td>\n",
       "      <td>0.289545</td>\n",
       "      <td>0.579090</td>\n",
       "      <td>0.220054</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>147</th>\n",
       "      <td>0.716539</td>\n",
       "      <td>0.330710</td>\n",
       "      <td>0.573231</td>\n",
       "      <td>0.220474</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>148</th>\n",
       "      <td>0.674671</td>\n",
       "      <td>0.369981</td>\n",
       "      <td>0.587616</td>\n",
       "      <td>0.250281</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>149</th>\n",
       "      <td>0.690259</td>\n",
       "      <td>0.350979</td>\n",
       "      <td>0.596665</td>\n",
       "      <td>0.210588</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>150 rows × 4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "    sepal_length sepal_width petal_length petal_width\n",
       "0       0.803773    0.551609     0.220644    0.031521\n",
       "1       0.828133    0.507020     0.236609    0.033801\n",
       "2       0.805333    0.548312     0.222752    0.034269\n",
       "3       0.800030    0.539151     0.260879    0.034784\n",
       "4       0.790965    0.569495     0.221470    0.031639\n",
       "..           ...         ...          ...         ...\n",
       "145     0.721557    0.323085     0.560015    0.247699\n",
       "146     0.729654    0.289545     0.579090    0.220054\n",
       "147     0.716539    0.330710     0.573231    0.220474\n",
       "148     0.674671    0.369981     0.587616    0.250281\n",
       "149     0.690259    0.350979     0.596665    0.210588\n",
       "\n",
       "[150 rows x 4 columns]"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.DataFrame(norm,columns=[['sepal_length', 'sepal_width', 'petal_length', 'petal_width']])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ce5d017-1fb1-418d-b8be-3f5b6707e044",
   "metadata": {},
   "source": [
    "Q3. What is PCA (Principle Component Analysis), and how is it used in dimensionality reduction? Provide an\n",
    "example to illustrate its application."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7380edc4-9462-4e32-8f49-ad6b0623b1fc",
   "metadata": {},
   "source": [
    "Ans:PCA stands for Principal Component Analysis. It is a widely used statistical technique in data analysis and dimensionality reduction. PCA aims to transform a dataset of possibly correlated variables into a new set of uncorrelated variables called principal components. These principal components are linear combinations of the original variables and are ordered in terms of the amount of variance they explain in the data.\n",
    "Here is how PCA is used in dimensionality reduction:-\n",
    "1.Standardised the data\n",
    "2.Compute the covariance matrix\n",
    "3.Compute the eigenvectors and eigenvalues\n",
    "4.Sort eigenvalues\n",
    "5.Select principal components\n",
    "and then \n",
    "6.Transform the data\n",
    "\n",
    "Finally ,the transformed dataset, consisting of the reduced number of principal components, can be used for further analysis or modeling tasks. By selecting the top k principal components that explain the most variance, you retain the most important information while discarding less significant features. This helps in reducing the dimensionality of the dataset, making it more manageable, and potentially improving the performance of subsequent analysis or machine learning algorithms."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "0313cadd-2573-4c2f-9146-061b46e79e3d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Explained Variance Ratio: [1. 0.]\n",
      "Principal Components:\n",
      "PC 1 : [-0.57735027 -0.57735027 -0.57735027]\n",
      "PC 2 : [ 0.         -0.70710678  0.70710678]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.decomposition import PCA\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import numpy as np\n",
    "scaler = StandardScaler()\n",
    "X = np.array([[1, 2, 3],[4, 5, 6],[7, 8, 9]])\n",
    "X_scaled = scaler.fit_transform(X)\n",
    "pca = PCA(n_components=2)  \n",
    "principal_components = pca.fit_transform(X_scaled)\n",
    "\n",
    "# Access the results\n",
    "explained_variance_ratio = pca.explained_variance_ratio_  \n",
    "principal_components = pca.components_  \n",
    "print(\"Explained Variance Ratio:\", explained_variance_ratio)\n",
    "\n",
    "# Print the principal components\n",
    "print(\"Principal Components:\")\n",
    "for i, component in enumerate(principal_components):\n",
    "    print(\"PC\", i+1, \":\", component)\n",
    "transformed_data = pca.transform(X_scaled)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "feef9d99-8a01-496b-9179-7f6ce6a98d19",
   "metadata": {},
   "source": [
    "Q4. What is the relationship between PCA and Feature Extraction, and how can PCA be used for Feature\n",
    "Extraction? Provide an example to illustrate this concept."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "750ad39a-b4b9-4566-8bda-83ca6abcc960",
   "metadata": {},
   "source": [
    "Ans:Principal component analysis (PCA) is an unsupervised linear transformation technique which is primarily used for feature extraction and dimensionality reduction.\n",
    "The relationship between PCA and Feature Extraction is:\n",
    "1.Dimensionality reduction\n",
    "2.Information preservation\n",
    "3.Reducing correlation\n",
    "PCA serves as a powerful tool for feature extraction by reducing the dimensionality, capturing the most important information, and reducing feature correlation. By transforming the original features into principal components, PCA provides a new set of features that can be used in subsequent analysis or modeling tasks, potentially improving interpretability, reducing noise, or enhancing the performance of machine learning algorithms."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "1c5acbcd-0bb9-475f-98a5-f3920794b5c3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Explained Variance Ratio: [1. 0.]\n",
      "Principal Components:\n",
      "PC 1 : [-0.57735027 -0.57735027 -0.57735027]\n",
      "PC 2 : [ 0.         -0.70710678  0.70710678]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.decomposition import PCA\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import numpy as np\n",
    "scaler = StandardScaler()\n",
    "X = np.array([[1, 2, 3],[4, 5, 6],[7, 8, 9]])\n",
    "X_scaled = scaler.fit_transform(X)\n",
    "pca = PCA(n_components=2)  \n",
    "principal_components = pca.fit_transform(X_scaled)\n",
    "\n",
    "# Access the results\n",
    "explained_variance_ratio = pca.explained_variance_ratio_  \n",
    "principal_components = pca.components_  \n",
    "print(\"Explained Variance Ratio:\", explained_variance_ratio)\n",
    "\n",
    "# Print the principal components\n",
    "print(\"Principal Components:\")\n",
    "for i, component in enumerate(principal_components):\n",
    "    print(\"PC\", i+1, \":\", component)\n",
    "transformed_data = pca.transform(X_scaled)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fcad21e3-78d0-4174-89fd-4e38229d0be8",
   "metadata": {},
   "source": [
    "Q5. You are working on a project to build a recommendation system for a food delivery service. The dataset\n",
    "contains features such as price, rating, and delivery time. Explain how you would use Min-Max scaling to\n",
    "preprocess the data."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4be7541b-dfa0-4a80-bee5-5a99a69c3def",
   "metadata": {},
   "source": [
    "1st of all ,identify the features: Review the dataset and identify the relevant features that you want to use for your recommendation system. In this case, the features could include price, rating, and delivery time.\n",
    "Normalize the features using Min-Max scaling: Apply Min-Max scaling to each feature individually.\n",
    "The formula will be :\n",
    "X_scaled = (X - X_min) / (X_max - X_min)\n",
    "where, X represents the original value of the feature.\n",
    "X_min is the minimum value of the feature in the dataset.\n",
    "X_max is the maximum value of the feature in the dataset.\n",
    "\n",
    "By subtracting the minimum value and dividing by the range (difference between maximum and minimum values), you can rescale the feature to a range between 0 and 1.\n",
    "\n",
    "Apply Min-Max scaling to each feature: Compute the minimum and maximum values for each feature in the dataset. Then, for each data point, apply the Min-Max scaling formula to obtain the scaled value for that feature.\n",
    "\n",
    "Update the dataset: Replace the original values of each feature with their corresponding scaled values. This will ensure that all the features are on a similar scale, with values between 0 and 1.\n",
    "After applying Min-Max scaling, the dataset will have the features transformed to a common range, allowing you to make meaningful comparisons and calculations based on these normalized values. This preprocessing step is particularly important when working with features that have different scales or units, such as price, rating, and delivery time, as it helps to avoid biases in the recommendation system that may arise due to the original magnitudes of the features."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "168838e2-4216-4f57-9e29-86817578486d",
   "metadata": {},
   "source": [
    "Q6. You are working on a project to build a model to predict stock prices. The dataset contains many\n",
    "features, such as company financial data and market trends. Explain how you would use PCA to reduce the\n",
    "dimensionality of the dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad811b7f-1017-403f-bb61-5ede3c830865",
   "metadata": {},
   "source": [
    "Ans:When building a model to predict stock prices with a dataset containing numerous features, PCA (Principal Component Analysis) can be used to reduce the dimensionality of the dataset. Dimensionality reduction is particularly useful when dealing with high-dimensional datasets as it helps to simplify the data and potentially improve model performance by eliminating redundant or less significant features.\n",
    "\n",
    "Here's how you could use PCA to reduce the dimensionality of the dataset for your stock price prediction model:\n",
    "\n",
    "Preprocess the dataset: Before applying PCA, it's essential to preprocess the dataset. This typically involves handling missing values, standardizing the features, and normalizing the data. Standardization and normalization ensure that the features are on a similar scale, which is crucial for PCA.\n",
    "\n",
    "Perform PCA: Apply PCA to the preprocessed dataset. The steps involved in performing PCA were explained earlier, but here's a recap:\n",
    "\n",
    "a. Standardize the data: If not already done in the preprocessing step, standardize the features to have zero mean and unit variance.\n",
    "\n",
    "b. Compute the covariance matrix: Calculate the covariance matrix of the standardized features. The covariance matrix represents the relationships between the features and provides insights into their co-variation.\n",
    "\n",
    "c. Compute the eigenvectors and eigenvalues: Obtain the eigenvectors and eigenvalues of the covariance matrix. The eigenvectors represent the principal components, and the eigenvalues represent the amount of variance explained by each principal component.\n",
    "\n",
    "d. Select the number of principal components: Determine the number of principal components you want to retain based on the desired level of dimensionality reduction. You can use techniques like scree plots or cumulative explained variance to assist in this decision.\n",
    "\n",
    "e. Transform the data: Project the original dataset onto the selected principal components to obtain a new set of transformed features. Each data point will be represented by a combination of the principal components.\n",
    "\n",
    "Retain transformed features: Keep the transformed features obtained from the PCA step, corresponding to the selected number of principal components. These transformed features capture the most important information from the original features while reducing the dimensionality of the dataset.\n",
    "\n",
    "Train the model: Use the reduced-dimensional dataset obtained from PCA to train your stock price prediction model. This reduced dataset will contain a smaller number of features, making it computationally efficient and potentially reducing the risk of overfitting.\n",
    "\n",
    "By applying PCA to the dataset, you can identify the most important principal components that explain the majority of the variance in the data. These components capture the most significant patterns or trends present in the original features while discarding less significant or redundant information. As a result, PCA can help simplify the dataset, improve model performance, and potentially enhance interpretability by focusing on the most influential features for stock price prediction."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4474f649-6604-4d4b-86ef-a95d85d8840e",
   "metadata": {},
   "source": [
    "Q7. For a dataset containing the following values: [1, 5, 10, 15, 20], perform Min-Max scaling to transform the\n",
    "values to a range of -1 to 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "e0cfc70c-76cb-4b6d-b70d-bb34ebd7e5ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "x = [1,5,10,15,20]\n",
    "mean = np.mean(x)\n",
    "std = np.std(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "05cd9b0a-3f40-4ccc-aacb-2d88e97d1b99",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[-1.3541130616417665, -0.7653682522323028, -0.02943724047047308, 0.7064937712913566, 1.4424247830531862]\n"
     ]
    }
   ],
   "source": [
    "normalize_data = []\n",
    "for i in x:\n",
    "    z_score = (i-mean)/std\n",
    "    normalize_data.append(z_score)\n",
    "print(normalize_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30f423f7-02ee-41e2-99ff-08e8e6e2016f",
   "metadata": {},
   "source": [
    "Q8. For a dataset containing the following features: [height, weight, age, gender, blood pressure], perform\n",
    "Feature Extraction using PCA. How many principal components would you choose to retain, and why?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db06e684-ed1c-4f3d-8fa4-1a0b3734e058",
   "metadata": {},
   "source": [
    "Ans: To determine the number of principal components to retain in PCA for the given dataset with features [height, weight, age, gender, blood pressure], we would need to consider several factors, including the desired level of dimensionality reduction, the amount of variance explained by each component, and the specific goals of our analysis. Without specific information about the dataset and its characteristics, it is challenging to provide an exact number."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ccc8d40d-71ae-42b5-ae9f-e5346e12cd4e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a736973-70cf-4237-87e9-fb9250565157",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eeaff39f-050d-4d74-9002-bda525de1404",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
